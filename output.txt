<file_overview>
Total files: 8
Date generated: 2024-09-16 07:04:30
Folder Structure:
├── async_utilities/
│   └── include/
│       ├── AsyncExecutor.h
│       ├── CallbackDispatcher.h
│       ├── LockFreeList.h
│       ├── LockFreeQueue.h
│       ├── LockFreeStack.h
│       ├── TaskQueue.h
│       └── ThreadPool.h
└── src/
    └── example_usage.cpp


Files included:
- async_utilities\include\AsyncExecutor.h
- async_utilities\include\CallbackDispatcher.h
- async_utilities\include\LockFreeList.h
- async_utilities\include\LockFreeQueue.h
- async_utilities\include\LockFreeStack.h
- async_utilities\include\TaskQueue.h
- async_utilities\include\ThreadPool.h
- src\example_usage.cpp
</file_overview>

<file path="async_utilities\include\AsyncExecutor.h" lines="112" modified="2024-09-15 22:42:28">
//
// Created by maxim on 15.09.2024.
//
#pragma once

#include <iostream>

#include "ThreadPool.h"
#include "CallbackDispatcher.h"


template<typename T>
class AsyncExecutor {
public:
    using AsyncOperation = std::function<T()>;
    using Callback = std::function<void(T)>;

    AsyncExecutor(ThreadPool& threadPool, CallbackDispatcher& dispatcher)
        : m_threadPool(threadPool), m_dispatcher(dispatcher) {}


    void start(AsyncOperation operation, Callback callback) {
        std::thread::id current_thread_id = std::this_thread::get_id();
        m_threadPool.enqueue([this, op = std::move(operation), cb = std::move(callback), current_thread_id]() mutable {
            try {
                T result = op();
                m_dispatcher.post([cb = std::move(cb), result]() mutable {
                    try {
                        cb(result);
                    } catch (const std::exception& e) {
                        // Handle callback exception
                        std::cerr << "Callback exception: " << e.what() << std::endl;
                    }
                }, current_thread_id);
            } catch (const std::exception& e) {
                // Handle operation exception
                std::cerr << "Operation exception: " << e.what() << std::endl;
            }
        });
    }

    std::future<T> start(AsyncOperation operation) {
        auto promise = std::make_shared<std::promise<T>>();
        auto future = promise->get_future();
        m_threadPool.enqueue([op = std::move(operation), promise]() mutable {
            try {
                T result = op();
                promise->set_value(result);
            } catch (...) {
                promise->set_exception(std::current_exception());
            }
        });

        return future;
    }



private:
    ThreadPool& m_threadPool;
    CallbackDispatcher& m_dispatcher;
};


class VoidAsyncExecutor {
public:
    using VoidAsyncOp = std::function<void()>;
    using VoidCallback = std::function<void()>;

    VoidAsyncExecutor(ThreadPool& threadPool, CallbackDispatcher& dispatcher)
        : m_threadPool(threadPool), m_dispatcher(dispatcher) {}

    void start(VoidAsyncOp operation, VoidCallback callback) const {
        m_threadPool.enqueue([this, op = std::move(operation), cb = std::move(callback)]() mutable {
            try {
                op();
                m_dispatcher.post([cb = std::move(cb)]() mutable {
                    try {
                        cb();
                    } catch (const std::exception& e) {
                        // Handle callback exception
                        std::cerr << "Callback exception: " << e.what() << std::endl;
                    }
                });
            } catch (const std::exception& e) {
                // Handle operation exception
                std::cerr << "Operation exception: " << e.what() << std::endl;
            }
        });
    }

    std::future<void> start(VoidAsyncOp operation)
    {
        auto promise = std::make_shared<std::promise<void>>();
        auto future = promise->get_future();

        m_threadPool.enqueue([op = std::move(operation), promise]() mutable {
            try {
                op();
                promise->set_value();
            } catch (...) {
                promise->set_exception(std::current_exception());
            }
        });

        return future;
    }

private:
    ThreadPool& m_threadPool;
    CallbackDispatcher& m_dispatcher;
};
</file>

<file path="async_utilities\include\CallbackDispatcher.h" lines="80" modified="2024-09-15 21:24:18">
//
// Created by maxim on 15.09.2024.
//
#pragma once
#include <thread>
#include <queue>
#include <mutex>
#include <condition_variable>
#include <functional>

struct CallbackInfo {
    std::function<void()> task;
    std::thread::id thread_id;

    CallbackInfo(std::function<void()> t, std::thread::id id)
        : task(std::move(t)), thread_id(id) {}
};

class CallbackDispatcher {
public:
    CallbackDispatcher() : m_stopped(false) {}

    void post(std::function<void()> task, std::thread::id thread_id = std::thread::id()) {
        {
            std::lock_guard<std::mutex> lock(m_mutex);
            m_tasks.emplace(std::move(task), thread_id);
        }
        m_condition.notify_one();
    }

    void execute_pending(size_t max_tasks = std::numeric_limits<size_t>::max()) {
        std::thread::id current_thread_id = std::this_thread::get_id();
        std::queue<CallbackInfo> tasks_to_execute;

        {
            std::lock_guard<std::mutex> lock(m_mutex);
            while (!m_tasks.empty() && tasks_to_execute.size() < max_tasks) {
                if (m_tasks.front().thread_id == std::thread::id() ||
                    m_tasks.front().thread_id == current_thread_id) {
                    tasks_to_execute.push(std::move(m_tasks.front()));
                    m_tasks.pop();
                } else {
                    // If the task is for a different thread, move it to the back of the queue
                    m_tasks.push(std::move(m_tasks.front()));
                    m_tasks.pop();
                }
            }
        }

        while (!tasks_to_execute.empty()) {
            tasks_to_execute.front().task();
            tasks_to_execute.pop();
        }
    }

    void wait() {
        std::unique_lock<std::mutex> lock(m_mutex);
        if (m_tasks.empty()) {
            m_condition.wait(lock, [this]() { return !m_tasks.empty() || m_stopped; });
        }
    }

    void stop() {
        {
            std::lock_guard<std::mutex> lock(m_mutex);
            m_stopped = true;
        }
        m_condition.notify_all();
    }

    bool is_stopped() const {
        return m_stopped;
    }

private:
    std::queue<CallbackInfo> m_tasks;
    std::mutex m_mutex;
    std::condition_variable m_condition;
    bool m_stopped;
};
</file>

<file path="async_utilities\include\LockFreeList.h" lines="115" modified="2024-09-16 02:51:30">
//
// Created by maxim on 16.09.2024.
//

#pragma once
#include <atomic>
#include <memory>
#include <optional>

template <typename T>
class LockFreeList {
private:
    struct Node {
        std::shared_ptr<T> data;
        std::atomic<Node*> next;

        Node() : next(nullptr) {}
        Node(const T& value) : data(std::make_shared<T>(value)), next(nullptr) {}
    };

    std::atomic<Node*> head;

public:
    LockFreeList() : head(nullptr) {}

    ~LockFreeList() {
        Node* current = head.load(std::memory_order_relaxed);
        while (current) {
            Node* next = current->next.load(std::memory_order_relaxed);
            delete current;
            current = next;
        }
    }

    void insert_after(const T& value, const T& after_value) {
        Node* new_node = new Node(value);
        Node* current = head.load(std::memory_order_acquire);

        while (true) {
            while (current && *(current->data) != after_value) {
                current = current->next.load(std::memory_order_acquire);
            }

            if (!current) {
                delete new_node;
                return;  // Value not found, insertion failed
            }

            new_node->next.store(current->next.load(std::memory_order_relaxed), std::memory_order_relaxed);

            if (current->next.compare_exchange_weak(new_node->next.load(std::memory_order_relaxed), new_node,
                                                    std::memory_order_release, std::memory_order_relaxed)) {
                return;  // Insertion successful
            }
        }
    }

    void insert_beginning(const T& value) {
        Node* new_node = new Node(value);
        Node* old_head = head.load(std::memory_order_relaxed);

        do {
            new_node->next.store(old_head, std::memory_order_relaxed);
        } while (!head.compare_exchange_weak(old_head, new_node,
                                             std::memory_order_release, std::memory_order_relaxed));
    }

    bool remove(const T& value) {
        Node* current = head.load(std::memory_order_acquire);
        Node* prev = nullptr;

        while (current && *(current->data) != value) {
            prev = current;
            current = current->next.load(std::memory_order_acquire);
        }

        if (!current) {
            return false;  // Value not found
        }

        Node* next = current->next.load(std::memory_order_acquire);

        if (prev) {
            if (!prev->next.compare_exchange_strong(current, next,
                                                    std::memory_order_release,
                                                    std::memory_order_relaxed)) {
                return false;  // Node has been removed by another thread
                                                    }
        } else {
            if (!head.compare_exchange_strong(current, next,
                                              std::memory_order_release,
                                              std::memory_order_relaxed)) {
                return false;  // Head has been changed by another thread
                                              }
        }

        // Defer deletion to avoid race condition
        std::atomic_thread_fence(std::memory_order_acquire);
        delete current;
        return true;
    }

    std::optional<T> find(const T& value) const {
        Node* current = head.load(std::memory_order_acquire);

        while (current) {
            if (*(current->data) == value) {
                return *(current->data);
            }
            current = current->next.load(std::memory_order_acquire);
        }

        return std::nullopt;
    }
};
</file>

<file path="async_utilities\include\LockFreeQueue.h" lines="77" modified="2024-09-16 02:50:53">
#pragma once
#include <atomic>
#include <memory>

template <typename T>
class LockFreeQueue {
private:
    struct Node {
        std::shared_ptr<T> data;
        std::atomic<Node*> next;

        Node() : next(nullptr) {}
        explicit Node(const T& value) : data(std::make_shared<T>(value)), next(nullptr) {}
    };

    std::atomic<Node*> head;
    std::atomic<Node*> tail;

public:
    LockFreeQueue() {
        Node* dummy = new Node();
        head.store(dummy);
        tail.store(dummy);
    }

    ~LockFreeQueue() {
        while (Node* old_head = head.load()) {
            head.store(old_head->next);
            delete old_head;
        }
    }

    void enqueue(const T& value) {
        Node* new_node = new Node(value);
        while (true) {
            Node* old_tail = tail.load();
            Node* next = old_tail->next.load();
            if (old_tail == tail.load()) {
                if (next == nullptr) {
                    if (old_tail->next.compare_exchange_weak(next, new_node)) {
                        tail.compare_exchange_strong(old_tail, new_node);
                        return;
                    }
                } else {
                    tail.compare_exchange_weak(old_tail, next);
                }
            }
        }
    }

    bool dequeue(T& result) {
        while (true) {
            Node* old_head = head.load(std::memory_order_acquire);
            Node* old_tail = tail.load(std::memory_order_acquire);
            Node* next = old_head->next.load(std::memory_order_acquire);

            if (old_head == head.load(std::memory_order_acquire)) {
                if (old_head == old_tail) {
                    if (next == nullptr) {
                        return false;  // Queue is empty
                    }
                    tail.compare_exchange_weak(old_tail, next, std::memory_order_release, std::memory_order_relaxed);
                } else {
                    if (next) {
                        result = *(next->data);
                        if (head.compare_exchange_weak(old_head, next, std::memory_order_release, std::memory_order_relaxed)) {
                            // Defer deletion to avoid race condition
                            std::atomic_thread_fence(std::memory_order_acquire);
                            delete old_head;
                            return true;
                        }
                    }
                }
            }
        }
    }
};
</file>

<file path="async_utilities\include\LockFreeStack.h" lines="63" modified="2024-09-16 02:56:45">
//
// Created by maxim on 16.09.2024.
//

#pragma once
#include <atomic>
#include <memory>
#include <optional>

template <typename T>
class LockFreeStack {
private:
    struct Node {
        std::shared_ptr<T> data;
        std::atomic<Node*> next;

        Node(const T& value) : data(std::make_shared<T>(value)), next(nullptr) {}
    };

    std::atomic<Node*> top;

public:
    LockFreeStack() : top(nullptr) {}

    ~LockFreeStack() {
        while (Node* old_top = top.load(std::memory_order_relaxed)) {
            top.store(old_top->next.load(std::memory_order_relaxed), std::memory_order_relaxed);
            delete old_top;
        }
    }

    void push(const T& value) {
        Node* new_node = new Node(value);
        Node* old_top = top.load(std::memory_order_relaxed);
        do {
            new_node->next.store(old_top, std::memory_order_relaxed);
        } while (!top.compare_exchange_weak(old_top, new_node,
                                            std::memory_order_release,
                                            std::memory_order_relaxed));
    }

    std::optional<T> pop() {
        Node* old_top = top.load(std::memory_order_relaxed);
        Node* new_top;
        do {
            if (old_top == nullptr) {
                return std::nullopt;  // Stack is empty
            }
            new_top = old_top->next.load(std::memory_order_relaxed);
        } while (!top.compare_exchange_weak(old_top, new_top,
                                            std::memory_order_release,
                                            std::memory_order_relaxed));

        std::optional<T> result = *(old_top->data);
        std::atomic_thread_fence(std::memory_order_acquire);
        delete old_top;
        return result;
    }

    bool is_empty() const {
        return top.load(std::memory_order_relaxed) == nullptr;
    }
};
</file>

<file path="async_utilities\include\TaskQueue.h" lines="51" modified="2024-09-15 22:37:23">
//
// Created by maxim on 15.09.2024.
//
#pragma once
#include <future>
#include <atomic>
#include <memory>
#include <queue>


class TaskQueue {
public:
    using Task = std::function<void()>;

    void push(Task task) {
        {
            std::lock_guard<std::mutex> lock(m_mutex);
            m_tasks.push(std::move(task));
        }
        m_condition.notify_one();
    }

    bool pop(Task& task) {
        std::unique_lock<std::mutex> lock(m_mutex);

        m_condition.wait(lock, [this]() {
            return !m_tasks.empty() || m_stopped;
        });

        if (m_stopped && m_tasks.empty())
            return false;

        task = std::move(m_tasks.front());
        m_tasks.pop();
        return true;
    }

    void stop() {
        {
            std::lock_guard<std::mutex> lock(m_mutex);
            m_stopped = true;
        }
        m_condition.notify_all();
    }

private:
    std::queue<Task> m_tasks;
    std::mutex m_mutex;
    std::condition_variable m_condition;
    bool m_stopped = false;
};
</file>

<file path="async_utilities\include\ThreadPool.h" lines="44" modified="2024-09-16 06:57:28">
//
// Created by maxim on 15.09.2024.
//
#pragma once
#include <functional>
#include <thread>
#include <vector>
#include <queue>
#include <condition_variable>
#include <future>

#include "LockFreeQueue.h"

class ThreadPool {
public:
    using Task = std::function<void()>;
    explicit ThreadPool(size_t threadCount = std::thread::hardware_concurrency()) : m_queue(), m_threads(threadCount) {
        for (auto& thread : m_threads) {
            thread = std::thread([this]() {
                Task task;
                while (m_queue.dequeue(task)) {
                    task();
                }
            });
        }
    }

    ~ThreadPool() {

        for (auto& thread : m_threads) {
            if (thread.joinable())
                thread.join();
        }
    }

    template<typename F>
    void enqueue(F&& task) {
        m_queue.enqueue(std::forward<F>(task));
    }

private:
    LockFreeQueue<Task> m_queue;
    std::vector<std::thread> m_threads;
};
</file>

<file path="src\example_usage.cpp" lines="55" modified="2024-09-15 21:59:34">
#include "AsyncExecutor.h"
#include <iostream>

int main() {
    std::cout << "Starting advanced asynchronous system..." << std::endl;

    // Create a thread pool
    ThreadPool threadPool;

    // Create a dispatcher
    CallbackDispatcher dispatcher;

    // Create an AsyncExecutor instance
    AsyncExecutor<int> asyncExecutor(threadPool, dispatcher);

    // Define the completion handler
    auto completionHandler = [](int result) {
        std::cout << "Asynchronous operation completed with result: " << result << std::endl;
    };

    // Start multiple asynchronous operations
    for (int i = 1; i < 6; ++i) {
        asyncExecutor.start([i]() {
            std::this_thread::sleep_for(std::chrono::seconds(2));
            return i * i;
        }, completionHandler);
    }

    std::cout << "Asynchronous operations initiated. Main thread is free to continue..." << std::endl;

    std::vector<std::future<int>> futures;
    for (int i = 6; i < 11; ++i) {
        futures.push_back(asyncExecutor.start([i]() {
            std::this_thread::sleep_for(std::chrono::seconds(4));
            return i * i;
        }));
    }

    // Later, collect results
    for (auto& future : futures) {
        int result = future.get(); // Will wait until the result is available
        std::cout << "Future result: " << result << std::endl;
    }
    dispatcher.execute_pending();

    while (!dispatcher.is_stopped()) {
        dispatcher.stop();
        std::this_thread::sleep_for(std::chrono::milliseconds(100));
    }

    // Destructor of ThreadPool will wait for all tasks to complete
    std::cout << "Main thread completed." << std::endl;
    return 0;
}
</file>



